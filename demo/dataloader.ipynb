{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from torch_connectomics.utils.seg.aff_util import affinitize, seg_to_affgraph\n",
    "from torch_connectomics.utils.seg.seg_util import widen_border, mknhood3d\n",
    "from torch_connectomics.data.dataset import AffinityDataset\n",
    "from torch_connectomics.data.utils import collate_fn\n",
    "from torch_connectomics.data.augmentation import *\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imgs(imgs, label=None, cmap=None):\n",
    "    plt.figure(figsize=(16,4))\n",
    "    for i in range(1,5):\n",
    "        plt.subplot('14%d' % (i))\n",
    "        if cmap is not None:\n",
    "            plt.imshow(imgs[i-1], cmap=cmap)\n",
    "        else:\n",
    "            plt.imshow(imgs[i-1])\n",
    "        if label is not None:\n",
    "            plt.title(label+' '+str(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data\n",
    "\n",
    "For Harvard Research Computing (RC) cluster users, you can directly access the dsta directory if you have access to the `coxfs01` partition. For external users please check the tutotial for downloading the dataset and change the `data_path` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1024, 1024) 3 uint8\n",
      "(100, 1024, 1024) 3 uint16\n"
     ]
    }
   ],
   "source": [
    "data_path = '/n/coxfs01/zudilin/data/SNEMI3D/'\n",
    "image_path = data_path + 'train_image.h5'\n",
    "label_path = data_path + 'train_label.h5'\n",
    "image = np.array(h5py.File(image_path, 'r')['main'])\n",
    "label = np.array(h5py.File(label_path, 'r')['main'])\n",
    "print(image.shape, image.ndim, image.dtype)\n",
    "print(label.shape, label.ndim, label.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size required for the augmentor: [  8 477 477]\n",
      "data augmentation:  True\n"
     ]
    }
   ],
   "source": [
    "model_io_size = (8, 256, 256)\n",
    "# setup augmentor\n",
    "augmentor = Compose([Rotate(p=1.0),\n",
    "                        Rescale(p=0.5),\n",
    "                        Flip(p=1.0),\n",
    "                        Elastic(alpha=10.0, p=0.5),\n",
    "                        Grayscale(p=0.75)], \n",
    "                        input_size = model_io_size)\n",
    "\n",
    "print('data augmentation: ', augmentor is not None)\n",
    "\n",
    "dataset = AffinityDataset(volume=[image / 255.0], label=[label], sample_input_size=augmentor.sample_size,\n",
    "                          sample_label_size=augmentor.sample_size, augmentor=augmentor, mode = 'train')    \n",
    "img_loader =  torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=8, shuffle=True, collate_fn = collate_fn,\n",
    "        num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration, (_, volume, label, class_weight, _) in enumerate(img_loader):\n",
    "    if iteration==0:\n",
    "        print(volume.size(), label.size())\n",
    "        show_imgs(volume[0, 0, 2:6].detach().numpy(), 'image', cmap='gray')\n",
    "        show_imgs(label[0, 0, 2:6].detach().numpy(), 'z_aff')\n",
    "        show_imgs(label[0, 1, 2:6].detach().numpy(), 'y_aff')\n",
    "        show_imgs(label[0, 2, 2:6].detach().numpy(), 'x_aff')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
